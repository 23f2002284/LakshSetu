{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d39b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Any, TypedDict\n",
    "\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"langgraph is required for agent graph orchestration. Install with: pip install langgraph\"\n",
    "    ) from e\n",
    "\n",
    "from schemas import (\n",
    "    UserProfile,\n",
    "    GitHubUserExtract,\n",
    "    LinkedInProfileExtract,\n",
    "    HuggingFaceModelExtract,\n",
    "    CareerActionRecommendation,\n",
    ")\n",
    "from trend_scrapping_node import process_extractions_and_recommend\n",
    "from User_interaction_node import run_interaction\n",
    "\n",
    "\n",
    "# Optional pluggable callbacks (override from your app before building the graph)\n",
    "RUN_USER_PROCESSING_CB: Optional[\n",
    "    Callable[[UserProfile], Tuple[UserProfile, Optional[GitHubUserExtract], Optional[LinkedInProfileExtract], Optional[List[HuggingFaceModelExtract]]]]\n",
    "] = None\n",
    "\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    user: UserProfile\n",
    "    github: Optional[GitHubUserExtract]\n",
    "    linkedin: Optional[LinkedInProfileExtract]\n",
    "    hf_models: Optional[List[HuggingFaceModelExtract]]\n",
    "    recs: List[CareerActionRecommendation]\n",
    "    last_user_processing_at: Optional[datetime]\n",
    "    next_user_processing_at: Optional[datetime]\n",
    "    schedule_interval_days: int\n",
    "\n",
    "\n",
    "# --- Nodes ---\n",
    "\n",
    "def user_processing_node(state: AgentState) -> AgentState:\n",
    "    user = state[\"user\"]\n",
    "    # Invoke custom ingestion if provided; else no-op (keep previous extracts)\n",
    "    if RUN_USER_PROCESSING_CB is not None:\n",
    "        user, github, linkedin, hf_models = RUN_USER_PROCESSING_CB(user)\n",
    "        state[\"user\"] = user\n",
    "        state[\"github\"] = github\n",
    "        state[\"linkedin\"] = linkedin\n",
    "        state[\"hf_models\"] = hf_models\n",
    "\n",
    "    now = datetime.now()\n",
    "    state[\"last_user_processing_at\"] = now\n",
    "    interval_days = int(state.get(\"schedule_interval_days\", 7))\n",
    "    state[\"next_user_processing_at\"] = now + timedelta(days=interval_days)\n",
    "    return state\n",
    "\n",
    "\n",
    "def trend_scrapping_node(state: AgentState) -> AgentState:\n",
    "    user, github, linkedin, hf_models = (\n",
    "        state[\"user\"],\n",
    "        state.get(\"github\"),\n",
    "        state.get(\"linkedin\"),\n",
    "        state.get(\"hf_models\"),\n",
    "    )\n",
    "    aligned_user, recs = process_extractions_and_recommend(user, github=github, linkedin=linkedin, hf_models=hf_models)\n",
    "    state[\"user\"] = aligned_user\n",
    "    state[\"recs\"] = recs\n",
    "    return state\n",
    "\n",
    "\n",
    "def user_interaction_node(state: AgentState) -> AgentState:\n",
    "    user, github, linkedin, hf_models = (\n",
    "        state[\"user\"],\n",
    "        state.get(\"github\"),\n",
    "        state.get(\"linkedin\"),\n",
    "        state.get(\"hf_models\"),\n",
    "    )\n",
    "    # Reuse interaction flow; it internally handles approvals/tasks\n",
    "    result = run_interaction(user, github_extract=github, linkedin_extract=linkedin, hf_models=hf_models)\n",
    "    state[\"user\"] = result[\"updated_profile\"]\n",
    "    # Keep recommendations as context (could be refreshed next cycle)\n",
    "    state.setdefault(\"recs\", [])\n",
    "    return state\n",
    "\n",
    "\n",
    "# --- Routing helpers ---\n",
    "\n",
    "def route_after_interaction(state: AgentState) -> str:\n",
    "    \"\"\"Loop in interaction until the next scheduled processing time.\"\"\"\n",
    "    next_at = state.get(\"next_user_processing_at\")\n",
    "    now = datetime.now()\n",
    "    if next_at and now >= next_at:\n",
    "        return \"user_processing\"\n",
    "    return \"user_interaction\"\n",
    "\n",
    "\n",
    "# --- Graph factory ---\n",
    "\n",
    "def build_agent(user: UserProfile, schedule_interval_days: int = 7):\n",
    "    \"\"\"Build and compile the LangGraph app with initial state returned.\n",
    "\n",
    "    Returns: (app, initial_state)\n",
    "    \"\"\"\n",
    "    sg = StateGraph(AgentState)\n",
    "    sg.add_node(\"user_processing\", user_processing_node)\n",
    "    sg.add_node(\"trend_scrapping\", trend_scrapping_node)\n",
    "    sg.add_node(\"user_interaction\", user_interaction_node)\n",
    "\n",
    "    sg.set_entry_point(\"user_processing\")\n",
    "    sg.add_edge(\"user_processing\", \"trend_scrapping\")\n",
    "    sg.add_edge(\"trend_scrapping\", \"user_interaction\")\n",
    "    sg.add_conditional_edges(\n",
    "        \"user_interaction\",\n",
    "        route_after_interaction,\n",
    "        {\n",
    "            \"user_processing\": \"user_processing\",\n",
    "            \"user_interaction\": \"user_interaction\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    app = sg.compile()\n",
    "    init_state: AgentState = {\n",
    "        \"user\": user,\n",
    "        \"github\": None,\n",
    "        \"linkedin\": None,\n",
    "        \"hf_models\": None,\n",
    "        \"recs\": [],\n",
    "        \"last_user_processing_at\": None,\n",
    "        \"next_user_processing_at\": datetime.now(),  # run immediately\n",
    "        \"schedule_interval_days\": schedule_interval_days,\n",
    "    }\n",
    "    return app, init_state\n",
    "\n",
    "\n",
    "# Optional helper to step a limited number of times (to avoid infinite loops)\n",
    "def run_steps(app, state: AgentState, max_steps: int = 3):\n",
    "    for _ in range(max_steps):\n",
    "        state = app.invoke(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a63c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\praty\\Desktop\\shortnote\\resume projects\\LakshSetu\\LakshSetu\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UserProfile(id=1, email='biswajitpolai5@gmail.com', name='biswajitpolai', age=None, location='Berhampur,odisha ,India', github=None, linkedin=None, huggingface=None, x=None, website=None, certifications=[Certifications(title='Big Data Modeling and Management Systems', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='Data Analytics in Python', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='ABV-IIITM Credentials', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='IIT BBS credentials', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='Python for Data science', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None)], skills=None, projects=[], blogs=None, achievements=[\"Runner's up at Hack For Tomorrow Grand Finale\"], trending_skills=None, skill_gap_analysis=None, recommendations=None, network_opportunities=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hard_coded_examples import (\n",
    "    build_github_user_output_from_analysis,\n",
    "    github_profile_analysis,\n",
    "    build_linkedin_profile_output_from_parser,\n",
    "    linked_in_profile_parser,\n",
    ")\n",
    "from User_Processing_node import User_report\n",
    "\n",
    "# Build structured outputs from hard-coded examples\n",
    "github_user_output = build_github_user_output_from_analysis(github_profile_analysis)\n",
    "linked_in_user_output = build_linkedin_profile_output_from_parser(linked_in_profile_parser)\n",
    "\n",
    "# Synthesize a unified UserProfile (now normalizes skills and certifications)\n",
    "hard_coded_user_report = User_report(github_user_output, linked_in_user_output)\n",
    "\n",
    "# Display the result\n",
    "hard_coded_user_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d1bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<langgraph.graph.state.CompiledStateGraph at 0x232968996a0>,\n",
       " {'user': UserProfile(id=1, email='biswajitpolai5@gmail.com', name='biswajitpolai', age=None, location='Berhampur,odisha ,India', github=None, linkedin=None, huggingface=None, x=None, website=None, certifications=[Certifications(title='Big Data Modeling and Management Systems', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='Data Analytics in Python', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='ABV-IIITM Credentials', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='IIT BBS credentials', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None), Certifications(title='Python for Data science', issuer='LinkedIn', issued_date='', credential_id=None, tags=[], certificate_strength=None)], skills=None, projects=[], blogs=None, achievements=[\"Runner's up at Hack For Tomorrow Grand Finale\"], trending_skills=None, skill_gap_analysis=None, recommendations=None, network_opportunities=None),\n",
       "  'github': None,\n",
       "  'linkedin': None,\n",
       "  'hf_models': None,\n",
       "  'recs': [],\n",
       "  'last_user_processing_at': None,\n",
       "  'next_user_processing_at': datetime.datetime(2025, 9, 7, 12, 20, 15, 239255),\n",
       "  'schedule_interval_days': 7})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_agent(hard_coded_user_report, schedule_interval_days=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
